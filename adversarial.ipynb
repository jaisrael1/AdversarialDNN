{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(17)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download dataset and prepare dataloaders\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "full_train_dataset = datasets.CIFAR10(root='data', train=True, transform=train_transforms, download=True)\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(full_train_dataset, [40000, 10000])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', train=False, transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=2, shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_dataset, batch_size=BATCH_SIZE, num_workers=2, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, num_workers=2, shuffle=False)\n",
    "\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train(model, weight_decay=0):\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "\n",
    "    # Store losses to plot after training finishes\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        # Track training + validation loss\n",
    "        train_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass - compute predictions by passing input through model\n",
    "            output = model(data)\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            # Backpropogation: compute gradient of loss w/ respect to model parameters\n",
    "            loss.backward()\n",
    "            # Backpropogation: Update parameters using loss gradient\n",
    "            optimizer.step()\n",
    "            # Update train loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # Check accuracy on validation set to make sure we don't overfit\n",
    "        model.eval()\n",
    "        for data, target in validation_loader:\n",
    "            # Forward pass - compute predictions by passing input through model\n",
    "            output = model(data)\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            # Update validation loss\n",
    "            validation_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # Calculate average train and validation losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        validation_loss = validation_loss/len(validation_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "            \n",
    "        # Display training and validation loss and accuracy every epoch \n",
    "        train_accuracy = get_accuracy(model, train_loader, DEVICE)\n",
    "        validation_accuracy = get_accuracy(model, validation_loader, DEVICE)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n",
    "            epoch, train_loss, validation_loss, train_accuracy, validation_accuracy))\n",
    "    return train_losses, validation_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1c187762543b7b51aa9f108e7f8545cb0c5a4f42c9141b7d72049c956d6c53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
